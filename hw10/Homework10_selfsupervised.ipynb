{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework10_selfsupervised.ipynb.txt",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0ad50f8097b74a879a7485d6a546a6ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5a06220435504c399b5a6493fe7b2761",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_03f0d2a2d5454008806435473176a3ec",
              "IPY_MODEL_fd6e35ed3bfc46c486953b085b1baf5a",
              "IPY_MODEL_56849deea3eb41f0a07c12bfbc542b3e"
            ]
          }
        },
        "5a06220435504c399b5a6493fe7b2761": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "03f0d2a2d5454008806435473176a3ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3dd70d4cf4cd4b2ba470ea7212bcc0e8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "  0%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1b597390e45d4771a00645f98885a7af"
          }
        },
        "fd6e35ed3bfc46c486953b085b1baf5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1301206197fa4c0eb574caab7e040da5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 10,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b6b92fc2b2bc42adb20fe32af41bf510"
          }
        },
        "56849deea3eb41f0a07c12bfbc542b3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e4d95c80dfda427888695cd93698fb93",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/10 [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d31adb8d0b6b4bf399f5e179b7c0a709"
          }
        },
        "3dd70d4cf4cd4b2ba470ea7212bcc0e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1b597390e45d4771a00645f98885a7af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1301206197fa4c0eb574caab7e040da5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b6b92fc2b2bc42adb20fe32af41bf510": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e4d95c80dfda427888695cd93698fb93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d31adb8d0b6b4bf399f5e179b7c0a709": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YUMVOLKOVA/dul_2021/blob/main/hw10/Homework10_selfsupervised.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZT2VPDHkxZa8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88310850-8d3e-40a0-dd86-ac84f16e6927"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'dul_2021'...\n",
            "remote: Enumerating objects: 384, done.\u001b[K\n",
            "remote: Counting objects: 100% (221/221), done.\u001b[K\n",
            "remote: Compressing objects: 100% (146/146), done.\u001b[K\n",
            "remote: Total 384 (delta 125), reused 99 (delta 68), pack-reused 163\u001b[K\n",
            "Receiving objects: 100% (384/384), 55.90 MiB | 34.44 MiB/s, done.\n",
            "Resolving deltas: 100% (182/182), done.\n",
            "Processing ./dul_2021\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Building wheels for collected packages: dul-2021\n",
            "  Building wheel for dul-2021 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dul-2021: filename=dul_2021-0.1.0-py3-none-any.whl size=27640 sha256=9cc6213bb1080b459a1d4fb9d9432e1d52c5150580bff6a2d10356229c625629\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-dmb1ixf9/wheels/55/59/29/0fb1c635652157734f4d741f32fc11979149684e83e919de06\n",
            "Successfully built dul-2021\n",
            "Installing collected packages: dul-2021\n",
            "Successfully installed dul-2021-0.1.0\n"
          ]
        }
      ],
      "source": [
        "!if [ -d dul_2021 ]; then rm -Rf dul_2021; fi\n",
        "!git clone https://github.com/GrigoryBartosh/dul_2021\n",
        "!pip install ./dul_2021"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dul_2021.utils.hw10_utils import *"
      ],
      "metadata": {
        "id": "kvPNepG2L2Hp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1. Context Encoder"
      ],
      "metadata": {
        "id": "86iMf3mEPaCR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRyHjNsSDzPY"
      },
      "source": [
        "Here we will implement [context encoder](https://arxiv.org/abs/1604.07379). The context encoder structures its self-supervised learning task by inpainting masked images. For example, the figure below shows different masking shapes, such as center masking, random block masking, and segmentation masking. Note that segmentation masking (c) is not purely self-supervised since we would need to train a image segmentation model which requires labels. However, the other two masking schemes (a) and (b) and purely self-supervised.\n",
        "\n",
        "![](https://drive.google.com/uc?id=1fhzkULYTtyMGUUF2n9dlPayJSdcY5pRv)\n",
        "\n",
        "More formally, the context encoder optimizes the following reconstruction loss:\n",
        "$$\\mathcal{L}_{rec} = \\left\\Vert \\hat{M} \\odot (x - F((1 - \\hat{M})\\odot x)) \\right\\Vert^2_2$$\n",
        "where $\\hat{M}$ is the masked region, $x$ is the image, and $F$ is the context encoder that tries to reconstruct the masked portion. In addition to the reconstruction loss, the paper introduces an adversarial loss that encourages more realistic inpaintings.\n",
        "$$L_{adv} = \\max_D \\mathbb{E}_{x\\in \\chi} [\\log(D(x)) + \\log(1 - D(F((1-\\hat{M})\\odot x)))]$$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this task we will crop central 14x14 region. You can use slightly afjusted architectures from AVB task from homework 8.\n",
        "\n",
        "**Hyperparametrs**\n",
        "\n",
        "* latent_dim = 128\n",
        "* epochs ~ 10-20\n",
        "* classifier need fewer updates than encoder-decoder part. We suggest to update it on each 10-th iteration.\n",
        "\n",
        "**You will provide the following deliverables**\n",
        "\n",
        "\n",
        "1. Over the course of training, record the mse loss and adversarial losses per batch.\n",
        "3. 30 (1, 28, 28) images. Where first 10 images are random sample from testdata with removed central region. Next 10 images are reconstracted images with your trained model. Last 10 images are initial without any removal."
      ],
      "metadata": {
        "id": "uW2dsZBKEo8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as opt\n",
        "from torch.optim import Adam\n",
        "import numpy as np\n",
        "\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "DoWaeNBNdDvn"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "NXuQ3wn6dTOJ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, latent_dim):\n",
        "    super().__init__()\n",
        "    self.latent_dim = latent_dim\n",
        "\n",
        "    self.seq = nn.Sequential(nn.Conv2d(1, 32, 3, 1, 1),\n",
        "                              nn.ReLU(),\n",
        "                              nn.Conv2d(32, 64, 3, 2, 1),\n",
        "                              nn.ReLU(), \n",
        "                              nn.Conv2d(64, 128, 3, 2, 1), \n",
        "                              nn.ReLU(), \n",
        "                              nn.Conv2d(128, 128, 3, 2, 1), \n",
        "                              nn.ReLU())\n",
        "    self.linear = nn.Linear(4 * 4 * 128, self.latent_dim)\n",
        "  def forward(self, data):\n",
        "    data = self.seq(data).flatten(start_dim=1)\n",
        "    output = self.linear(data)\n",
        "    return output\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, latent_dim):\n",
        "    super().__init__()\n",
        "    self.latent_dim = latent_dim\n",
        "\n",
        "    self.seq = nn.Sequential(nn.ConvTranspose2d(128, 128, 3, 2, 1), \n",
        "                              nn.ReLU(), \n",
        "                              nn.ConvTranspose2d(128, 64, 4, 2, 1), \n",
        "                              nn.ReLU(), \n",
        "                              nn.ConvTranspose2d(64, 32, 4, 2, 1), \n",
        "                              nn.ReLU(), \n",
        "                              nn.Conv2d(32, 1, 3, 1, 1),\n",
        "                              nn.Tanh())\n",
        "    self.linaer = nn.Sequential(nn.Linear(self.latent_dim, 4 * 4 * 128),\n",
        "                                nn.ReLU())\n",
        "  def forward(self, data):\n",
        "    data = self.linaer(data).view(-1, 128, 4, 4)\n",
        "    output = self.seq(data)\n",
        "    return output\n",
        "    "
      ],
      "metadata": {
        "id": "ArG5l_Q3dmIF"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class D(nn.Module):\n",
        "  def __init__(self, hidden_dim): \n",
        "    super().__init__()\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.seq = nn.Sequential(nn.Conv2d(1, 32, 3, 1, 1),\n",
        "                              nn.ReLU(),\n",
        "                              nn.Conv2d(32, 64, 3, 2, 1),\n",
        "                              nn.ReLU(), \n",
        "                              nn.Conv2d(64, 128, 3, 2, 1), \n",
        "                              nn.ReLU(), \n",
        "                              nn.Conv2d(128, 128, 3, 2, 1), \n",
        "                              nn.ReLU())\n",
        "    self.linear = nn.Sequential(nn.Linear(4 * 4 * 128, self.hidden_dim), \n",
        "                                nn.ReLU(),\n",
        "                                nn.Linear(self.hidden_dim, self.hidden_dim), \n",
        "                                nn.ReLU(),\n",
        "                                nn.Linear(self.hidden_dim, 1),\n",
        "                                nn.Sigmoid())\n",
        "  def forward(self, data):\n",
        "    data = self.seq(data).flatten(start_dim=1)\n",
        "    output = self.linear(data)\n",
        "    return output \n"
      ],
      "metadata": {
        "id": "ZmOpYbR6ykFa"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Context_Encoder(nn.Module):\n",
        "  def __init__(self, latent_dim, hidden_dim):\n",
        "    super().__init__()\n",
        "    self.latent_dim = latent_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.encoder = Encoder(self.latent_dim)\n",
        "    self.decoder = Decoder(self.latent_dim)\n",
        "    self.d = D(self.hidden_dim)\n",
        "  \n",
        "  def creat_mask(self, data, height=14, width=14):\n",
        "    size = data.shape[0]\n",
        "    h, w = data.shape[2], data.shape[3]\n",
        "\n",
        "    init_mask = torch.zeros_like(data)\n",
        "    diff_h = int(h - height)\n",
        "    diff_w = int(w - width)\n",
        "    random_h = np.random.randint(0, diff_h, size)\n",
        "    random_w = np.random.randint(0, diff_w, size)\n",
        "\n",
        "    for m, h, w in zip(init_mask, random_h, random_w):\n",
        "      m[:, h:h+height, w:w+width] = 1\n",
        "    mask = init_mask.clone().detach()\n",
        "    return mask\n",
        "\n",
        "  def forward(self, data):\n",
        "    data = self.encoder(data)\n",
        "    output = self.decoder(data)\n",
        "    return output\n",
        "\n",
        "  def get_loss(self, data):\n",
        "    size = data.shape[0]\n",
        "    mask = self.creat_mask(data)\n",
        "    x = (1 - mask) * data\n",
        "    recon_data = self.forward(x)\n",
        "    recon_data_copy = recon_data.clone().detach()\n",
        "\n",
        "    loss_record = mask * F.mse_loss(data, recon_data)\n",
        "    loss_record = loss_record.reshape(size, -1).mean(1)\n",
        "    loss_record = loss_record.mean()\n",
        "\n",
        "    d_real = self.d(data)\n",
        "    ones_real = torch.ones_like(d_real)\n",
        "    loss_real = F.binary_cross_entropy(d_real, ones_real)\n",
        "\n",
        "    d_fake_1 = self.d(recon_data)    \n",
        "    ones_fake = torch.ones_like(d_fake_1)\n",
        "    loss_fake_1 = F.binary_cross_entropy(d_fake_1, ones_fake)\n",
        "\n",
        "    loss_rec_desc = loss_record + loss_fake_1\n",
        "    loss_rec_desc = loss_rec_desc.mean()\n",
        "\n",
        "    d_fake_2 = self.d(recon_data_copy)\n",
        "    zeroes_fake = torch.zeros_like(d_fake_2)\n",
        "    loss_fake_2 = F.binary_cross_entropy(d_fake_2, zeroes_fake)\n",
        "\n",
        "    loss_adversarial = loss_real + loss_fake_2\n",
        "    loss_adversarial = loss_adversarial.mean()\n",
        "\n",
        "    return loss_record, loss_adversarial, loss_rec_desc\n",
        "\n",
        "  def fit(self, train_data, lr, epochs):\n",
        "    self.train()\n",
        "    mse_losses, adversarial_losses = [], []\n",
        "    list_params_encoder = list(self.encoder.parameters())\n",
        "    list_params_decoder = list(self.decoder.parameters())\n",
        "    optim_e_d = Adam(list_params_encoder + list_params_decoder, lr=lr)\n",
        "    optim_disc = Adam(self.d.parameters(), lr=lr)    \n",
        "\n",
        "    i = 1\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "      for batch in train_data:\n",
        "        batch = batch.to(device)\n",
        "        loss_record, loss_adversarial, loss_rec_desc = self.get_loss(batch)\n",
        "\n",
        "        optim_e_d.zero_grad()\n",
        "        loss_rec_desc.backward()\n",
        "        optim_e_d.step()\n",
        "\n",
        "        if (i + 1) % 10 == 0:\n",
        "          optim_disc.zero_grad()\n",
        "          loss_adversarial.backward()\n",
        "          optim_disc.step()\n",
        "        i += 1\n",
        "\n",
        "        mse_losses.append(loss_record.item())\n",
        "        adversarial_losses.append(loss_adversarial.item())\n",
        "\n",
        "    mse_losses = np.array(mse_losses) \n",
        "    adversarial_losses = np.array(adversarial_losses)\n",
        "    return mse_losses, adversarial_losses\n",
        "\n",
        "  def get_reconstructions(self, data):\n",
        "    len_data = len(data)\n",
        "    part_data = data[np.random.choice(len_data, size=10, replace=False)]\n",
        "    self.eval()\n",
        "    data = torch.tensor(part_data)\n",
        "    data = data.to(device)\n",
        "    mask = self.creat_mask(data)\n",
        "    x = (1 - mask) * data\n",
        "    data_forward = self.forward(x)\n",
        "\n",
        "    removed_central_region = x.cpu().numpy()\n",
        "    reconstracted_images = data_forward.cpu().numpy()\n",
        "    initial = data.cpu().numpy()\n",
        "    combo = np.vstack((removed_central_region, reconstracted_images, initial))\n",
        "    return combo\n",
        "    \n"
      ],
      "metadata": {
        "id": "EU32SyP_1Gdv"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = {'latent_dim': 128 ,\n",
        "          'hidden_dim': 128,\n",
        "          'batch_size': 128,\n",
        "          'num_epochs': 10,\n",
        "          'lr': 1e-3}"
      ],
      "metadata": {
        "id": "dqnCbIooLmRr"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def q1(train_data, test_data):\n",
        "    \"\"\"\n",
        "    train_data: An (n_train, 1, 28, 28) torchvision dataset of MNIST images with values from -1 to 1\n",
        "    test_data: An (n_test, 1, 28, 28) torchvision dataset of MNIST images with values from -1 to 1\n",
        "\n",
        "    Returns\n",
        "    - a (# of training iterations, ) numpy array of full of mse losses\n",
        "    - a (# of training iterations, ) numpy array of full of adversarial losses\n",
        "    - a (30, 1, 28, 28) numpy array of 10 transformed images, 10 reconstructions, and 10 groundtruths\n",
        "    \"\"\"\n",
        "    train = train_data.data.unsqueeze(1) \n",
        "    train = 2.0 * (train / 255.0) - 1.0\n",
        "    train = DataLoader(train,\n",
        "                       batch_size=params['batch_size'],\n",
        "                       shuffle=True)\n",
        "    \n",
        "    model_CE = Context_Encoder(latent_dim=params['latent_dim'],\n",
        "                               hidden_dim=params['hidden_dim'])\n",
        "    model_CE = model_CE.to(device)\n",
        "    print('done with init')\n",
        "    mse_losses, adversarial_losses = model_CE.fit(train_data=train, \n",
        "                                                  lr=params['lr'], \n",
        "                                                  epochs=params['num_epochs'])\n",
        "    print('done with losses')\n",
        "    reconstructions = model_CE.get_reconstructions(test_data)\n",
        "    print('done with reconstruction')\n",
        "    return mse_losses, adversarial_losses, reconstructions"
      ],
      "metadata": {
        "id": "lQnVUrB36iFT"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q1_results(q1)"
      ],
      "metadata": {
        "id": "3uQzNlV8bQMB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68,
          "referenced_widgets": [
            "0ad50f8097b74a879a7485d6a546a6ca",
            "5a06220435504c399b5a6493fe7b2761",
            "03f0d2a2d5454008806435473176a3ec",
            "fd6e35ed3bfc46c486953b085b1baf5a",
            "56849deea3eb41f0a07c12bfbc542b3e",
            "3dd70d4cf4cd4b2ba470ea7212bcc0e8",
            "1b597390e45d4771a00645f98885a7af",
            "1301206197fa4c0eb574caab7e040da5",
            "b6b92fc2b2bc42adb20fe32af41bf510",
            "e4d95c80dfda427888695cd93698fb93",
            "d31adb8d0b6b4bf399f5e179b7c0a709"
          ]
        },
        "outputId": "af6af849-b4d7-4ebe-d9d2-e98b60b63349"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done with init\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0ad50f8097b74a879a7485d6a546a6ca",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "c7OSiq3ytd85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2. Rotations Prediction"
      ],
      "metadata": {
        "id": "W6LVLHMMjzVL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we will imlement this [paper](https://arxiv.org/abs/1803.07728). Here, model learns good representations for downstream tasks by proxy task of prediciting rotation of the original image.\n",
        "\n",
        "![](https://drive.google.com/uc?id=1eHXLH-N_6uMGRzdf1Wjnga26qlS5-FRv)\n",
        "\n",
        "We will work with same rotations as in paper (0, 90, 180, 270). You can use architecture AVB task in hw8. Latent dim 128 and 10 epochs should be enough.\n",
        "\n",
        "**You will provide the following deliverables**\n",
        "\n",
        "\n",
        "1. Over the course of training, record the loss per batch.\n",
        "2. Over the course of training, record the accuracy score for each iteration.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TUP2DAeeIK0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = {'latent_dim': 128,\n",
        "          'num_epochs': 10}"
      ],
      "metadata": {
        "id": "mZcGsBepevSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def q2(train_data):\n",
        "    \"\"\"\n",
        "    train_data: An (n_train, 1, 28, 28) torchvision dataset of MNIST images with values from -1 to 1\n",
        "    Returns\n",
        "    - a (# of training iterations, ) numpy array of full of losses\n",
        "    - a (# of training epochs, ) numpy array of full of accuracy scores\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "_YSch41UKy2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q2_results(q2)"
      ],
      "metadata": {
        "id": "VIpLkvcU39ib"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}